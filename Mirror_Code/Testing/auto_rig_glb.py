#!/usr/bin/env python3
"""
Create a lightweight autogenerated humanoid skin for a static GLB mesh.

This script is a fallback when DCC tooling (Blender/Maya) is unavailable.
It appends:
  - joint nodes (simple humanoid hierarchy)
  - inverse bind matrices
  - JOINTS_0 and WEIGHTS_0 vertex attributes
  - one skin entry

It is not a replacement for artist-authored rigging quality, but it creates
valid skinned data so runtime bone transforms and IK can be prototyped.
"""

from __future__ import annotations

import argparse
import json
import math
import struct
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np

GLB_MAGIC = 0x46546C67
GLB_VERSION = 2
JSON_CHUNK = 0x4E4F534A
BIN_CHUNK = 0x004E4942

COMPONENT_DTYPE = {
    5120: np.int8,
    5121: np.uint8,
    5122: np.int16,
    5123: np.uint16,
    5125: np.uint32,
    5126: np.float32,
}

TYPE_COMPONENTS = {
    "SCALAR": 1,
    "VEC2": 2,
    "VEC3": 3,
    "VEC4": 4,
    "MAT2": 4,
    "MAT3": 9,
    "MAT4": 16,
}


@dataclass
class JointSpec:
    name: str
    parent: int
    absolute_position: np.ndarray
    side: int  # -1 right, 0 center, +1 left


def parse_glb(path: Path) -> Tuple[Dict, bytearray]:
    raw = path.read_bytes()
    if len(raw) < 20:
        raise ValueError("Input file is too small to be a valid GLB.")

    magic, version, declared_len = struct.unpack_from("<III", raw, 0)
    if magic != GLB_MAGIC:
        raise ValueError("Invalid GLB magic.")
    if version != GLB_VERSION:
        raise ValueError(f"Unsupported GLB version: {version}")
    if declared_len != len(raw):
        raise ValueError("GLB declared length does not match file size.")

    offset = 12
    gltf = None
    bin_blob = bytearray()

    while offset + 8 <= len(raw):
        chunk_len, chunk_type = struct.unpack_from("<II", raw, offset)
        offset += 8
        data = raw[offset : offset + chunk_len]
        offset += chunk_len

        if chunk_type == JSON_CHUNK:
            text = data.decode("utf-8").rstrip(" \t\r\n\0")
            gltf = json.loads(text)
        elif chunk_type == BIN_CHUNK:
            bin_blob = bytearray(data)

    if gltf is None:
        raise ValueError("GLB JSON chunk was not found.")

    if "meshes" not in gltf or not gltf["meshes"]:
        raise ValueError("GLB has no meshes.")

    return gltf, bin_blob


def align4(blob: bytearray) -> None:
    pad = (-len(blob)) % 4
    if pad:
        blob.extend(b"\x00" * pad)


def accessor_array(gltf: Dict, bin_blob: bytearray, accessor_index: int) -> np.ndarray:
    accessor = gltf["accessors"][accessor_index]
    buffer_view = gltf["bufferViews"][accessor["bufferView"]]
    component_type = accessor["componentType"]
    dtype = COMPONENT_DTYPE[component_type]
    components = TYPE_COMPONENTS[accessor["type"]]
    count = int(accessor["count"])

    accessor_offset = int(accessor.get("byteOffset", 0))
    view_offset = int(buffer_view.get("byteOffset", 0))
    stride = int(buffer_view.get("byteStride", 0))
    start = view_offset + accessor_offset

    if stride and stride != np.dtype(dtype).itemsize * components:
        # Interleaved view.
        out = np.zeros((count, components), dtype=dtype)
        item_nbytes = np.dtype(dtype).itemsize * components
        for i in range(count):
            s = start + i * stride
            e = s + item_nbytes
            out[i] = np.frombuffer(bin_blob[s:e], dtype=dtype, count=components)
        return out

    arr = np.frombuffer(
        bin_blob,
        dtype=dtype,
        count=count * components,
        offset=start,
    )
    return arr.reshape((count, components))


def append_buffer_view_and_accessor(
    gltf: Dict,
    bin_blob: bytearray,
    payload: bytes,
    *,
    component_type: int,
    count: int,
    accessor_type: str,
    target: int | None = None,
) -> int:
    align4(bin_blob)
    byte_offset = len(bin_blob)
    bin_blob.extend(payload)

    buffer_views = gltf.setdefault("bufferViews", [])
    view = {
        "buffer": 0,
        "byteOffset": byte_offset,
        "byteLength": len(payload),
    }
    if target is not None:
        view["target"] = target
    buffer_views.append(view)
    view_idx = len(buffer_views) - 1

    accessors = gltf.setdefault("accessors", [])
    accessor = {
        "bufferView": view_idx,
        "componentType": component_type,
        "count": int(count),
        "type": accessor_type,
    }
    accessors.append(accessor)
    return len(accessors) - 1


def build_joint_specs(positions: np.ndarray) -> List[JointSpec]:
    mins = positions.min(axis=0)
    maxs = positions.max(axis=0)
    size = np.maximum(maxs - mins, 1e-6)

    # Infer axes from extents: tallest axis = up, next = left/right.
    axes = sorted(range(3), key=lambda i: size[i], reverse=True)
    up_axis = axes[0]
    lr_axis = axes[1]
    fb_axis = axes[2]

    center = (mins + maxs) * 0.5
    up_min = mins[up_axis]
    up_size = size[up_axis]
    lr_size = size[lr_axis]
    fb_size = size[fb_axis]

    def make_vec(lr_off: float, up_norm: float, fb_off: float = 0.0) -> np.ndarray:
        v = np.array(center, dtype=np.float32)
        v[up_axis] = float(up_min + up_norm * up_size)
        v[lr_axis] = float(center[lr_axis] + lr_off)
        v[fb_axis] = float(center[fb_axis] + fb_off)
        return v

    shoulder_off = 0.22 * lr_size
    elbow_off = 0.33 * lr_size
    wrist_off = 0.42 * lr_size
    hand_off = 0.47 * lr_size
    hip_off = 0.09 * lr_size
    foot_fb = 0.12 * fb_size

    joints = [
        JointSpec("Hips", -1, make_vec(0.0, 0.53), 0),
        JointSpec("Spine", 0, make_vec(0.0, 0.61), 0),
        JointSpec("Chest", 1, make_vec(0.0, 0.69), 0),
        JointSpec("Neck", 2, make_vec(0.0, 0.80), 0),
        JointSpec("Head", 3, make_vec(0.0, 0.90), 0),
        JointSpec("LeftShoulder", 2, make_vec(+0.16 * lr_size, 0.76), +1),
        JointSpec("LeftUpperArm", 5, make_vec(+shoulder_off, 0.74), +1),
        JointSpec("LeftLowerArm", 6, make_vec(+elbow_off, 0.71), +1),
        JointSpec("LeftHand", 7, make_vec(+wrist_off, 0.69), +1),
        JointSpec("RightShoulder", 2, make_vec(-0.16 * lr_size, 0.76), -1),
        JointSpec("RightUpperArm", 9, make_vec(-shoulder_off, 0.74), -1),
        JointSpec("RightLowerArm", 10, make_vec(-elbow_off, 0.71), -1),
        JointSpec("RightHand", 11, make_vec(-wrist_off, 0.69), -1),
        JointSpec("LeftUpperLeg", 0, make_vec(+hip_off, 0.50), +1),
        JointSpec("LeftLowerLeg", 13, make_vec(+hip_off, 0.30), +1),
        JointSpec("LeftFoot", 14, make_vec(+hand_off * 0.30, 0.08, +foot_fb), +1),
        JointSpec("RightUpperLeg", 0, make_vec(-hip_off, 0.50), -1),
        JointSpec("RightLowerLeg", 16, make_vec(-hip_off, 0.30), -1),
        JointSpec("RightFoot", 17, make_vec(-hand_off * 0.30, 0.08, +foot_fb), -1),
    ]
    return joints


def generate_skin_weights(
    positions: np.ndarray,
    joints: List[JointSpec],
) -> Tuple[np.ndarray, np.ndarray]:
    joint_positions = np.stack([j.absolute_position for j in joints], axis=0)
    dists = np.linalg.norm(positions[:, None, :] - joint_positions[None, :, :], axis=2)
    body_scale = float(np.max(np.ptp(positions, axis=0)))
    sigma = max(1e-6, 0.18 * body_scale)
    raw = np.exp(-((dists / sigma) ** 2)).astype(np.float32)

    mins = positions.min(axis=0)
    maxs = positions.max(axis=0)
    size = np.maximum(maxs - mins, 1e-6)
    axes = sorted(range(3), key=lambda i: size[i], reverse=True)
    lr_axis = axes[1]
    lr_center = float((mins[lr_axis] + maxs[lr_axis]) * 0.5)
    lr_half = float(size[lr_axis] * 0.5)
    lr_norm = (positions[:, lr_axis] - lr_center) / max(lr_half, 1e-6)
    side_mag = np.abs(lr_norm)

    right_joint_ids = [idx for idx, j in enumerate(joints) if j.side < 0]
    left_joint_ids = [idx for idx, j in enumerate(joints) if j.side > 0]

    left_mask = (lr_norm > 0.0) & (side_mag > 0.10)
    right_mask = (lr_norm < 0.0) & (side_mag > 0.10)
    if np.any(left_mask):
        raw[np.ix_(left_mask, right_joint_ids)] *= 0.20
    if np.any(right_mask):
        raw[np.ix_(right_mask, left_joint_ids)] *= 0.20

    # Keep top-4 influences required by JOINTS_0/WEIGHTS_0.
    top4_idx = np.argpartition(raw, kth=-4, axis=1)[:, -4:]
    top4_weights = np.take_along_axis(raw, top4_idx, axis=1)
    order = np.argsort(-top4_weights, axis=1)
    top4_idx = np.take_along_axis(top4_idx, order, axis=1).astype(np.uint16)
    top4_weights = np.take_along_axis(top4_weights, order, axis=1)

    weight_sum = np.sum(top4_weights, axis=1, keepdims=True)
    top4_weights = top4_weights / np.maximum(weight_sum, 1e-8)
    top4_weights = top4_weights.astype(np.float32)
    return top4_idx, top4_weights


def build_inverse_bind_matrices(joints: List[JointSpec]) -> np.ndarray:
    ibm_list = []
    for j in joints:
        mat = np.eye(4, dtype=np.float32)
        mat[0, 3] = -j.absolute_position[0]
        mat[1, 3] = -j.absolute_position[1]
        mat[2, 3] = -j.absolute_position[2]
        # glTF accessor matrices are column-major.
        ibm_list.append(mat.flatten(order="F"))
    return np.vstack(ibm_list).astype(np.float32)


def find_primary_mesh_and_node(gltf: Dict) -> Tuple[int, int, int]:
    meshes = gltf["meshes"]
    if not meshes:
        raise ValueError("No mesh found.")

    # Select the mesh with the highest vertex count.
    max_vertices = -1
    chosen_mesh = 0
    chosen_prim = 0
    for mi, mesh in enumerate(meshes):
        for pi, prim in enumerate(mesh.get("primitives", [])):
            pos_accessor = prim.get("attributes", {}).get("POSITION")
            if pos_accessor is None:
                continue
            count = int(gltf["accessors"][pos_accessor]["count"])
            if count > max_vertices:
                max_vertices = count
                chosen_mesh = mi
                chosen_prim = pi
    if max_vertices <= 0:
        raise ValueError("No primitive with POSITION accessor found.")

    nodes = gltf.get("nodes", [])
    mesh_node = -1
    for ni, node in enumerate(nodes):
        if node.get("mesh") == chosen_mesh:
            mesh_node = ni
            break
    if mesh_node < 0:
        raise ValueError("No node references the chosen mesh.")

    return chosen_mesh, chosen_prim, mesh_node


def add_joint_nodes(gltf: Dict, joints: List[JointSpec], attach_parent: int) -> List[int]:
    nodes = gltf.setdefault("nodes", [])
    start = len(nodes)

    # Build local translations from absolute bind positions.
    abs_positions = [j.absolute_position for j in joints]
    local_translations: List[np.ndarray] = []
    for j in joints:
        if j.parent < 0:
            local_translations.append(j.absolute_position.astype(np.float32))
        else:
            local = j.absolute_position - abs_positions[j.parent]
            local_translations.append(local.astype(np.float32))

    children_map: Dict[int, List[int]] = {i: [] for i in range(len(joints))}
    for idx, j in enumerate(joints):
        if j.parent >= 0:
            children_map[j.parent].append(idx)

    joint_node_indices = []
    for i, j in enumerate(joints):
        node = {
            "name": f"AutoRig_{j.name}",
            "translation": [float(x) for x in local_translations[i]],
        }
        if children_map[i]:
            node["children"] = [start + child_idx for child_idx in children_map[i]]
        nodes.append(node)
        joint_node_indices.append(start + i)

    parent_children = nodes[attach_parent].setdefault("children", [])
    if joint_node_indices[0] not in parent_children:
        parent_children.append(joint_node_indices[0])
    return joint_node_indices


def write_glb(path: Path, gltf: Dict, bin_blob: bytearray) -> None:
    if "buffers" not in gltf or not gltf["buffers"]:
        gltf["buffers"] = [{"byteLength": len(bin_blob)}]
    gltf["buffers"][0]["byteLength"] = len(bin_blob)

    json_payload = json.dumps(gltf, separators=(",", ":"), ensure_ascii=True).encode("utf-8")
    while len(json_payload) % 4:
        json_payload += b" "

    align4(bin_blob)
    bin_payload = bytes(bin_blob)

    total_len = 12 + 8 + len(json_payload) + 8 + len(bin_payload)
    header = struct.pack("<III", GLB_MAGIC, GLB_VERSION, total_len)
    json_chunk_header = struct.pack("<II", len(json_payload), JSON_CHUNK)
    bin_chunk_header = struct.pack("<II", len(bin_payload), BIN_CHUNK)

    path.write_bytes(header + json_chunk_header + json_payload + bin_chunk_header + bin_payload)


def main() -> None:
    parser = argparse.ArgumentParser(description="Autorig static GLB into a skinned GLB")
    parser.add_argument("--input", required=True, help="Path to input GLB")
    parser.add_argument("--output", required=True, help="Path to output GLB")
    args = parser.parse_args()

    in_path = Path(args.input).expanduser().resolve()
    out_path = Path(args.output).expanduser().resolve()

    gltf, bin_blob = parse_glb(in_path)
    mesh_idx, prim_idx, mesh_node_idx = find_primary_mesh_and_node(gltf)
    prim = gltf["meshes"][mesh_idx]["primitives"][prim_idx]
    pos_accessor_idx = prim["attributes"]["POSITION"]
    positions = accessor_array(gltf, bin_blob, pos_accessor_idx).astype(np.float32)

    joints = build_joint_specs(positions)
    joint_ids, joint_weights = generate_skin_weights(positions, joints)
    ibm = build_inverse_bind_matrices(joints)

    joints_accessor = append_buffer_view_and_accessor(
        gltf,
        bin_blob,
        joint_ids.tobytes(),
        component_type=5123,
        count=joint_ids.shape[0],
        accessor_type="VEC4",
        target=34962,
    )
    weights_accessor = append_buffer_view_and_accessor(
        gltf,
        bin_blob,
        joint_weights.tobytes(),
        component_type=5126,
        count=joint_weights.shape[0],
        accessor_type="VEC4",
        target=34962,
    )
    ibm_accessor = append_buffer_view_and_accessor(
        gltf,
        bin_blob,
        ibm.tobytes(),
        component_type=5126,
        count=ibm.shape[0],
        accessor_type="MAT4",
    )

    prim["attributes"]["JOINTS_0"] = joints_accessor
    prim["attributes"]["WEIGHTS_0"] = weights_accessor

    # Attach generated joints to parent of mesh node if available; else to scene root.
    attach_parent = None
    for idx, node in enumerate(gltf.get("nodes", [])):
        children = node.get("children", [])
        if mesh_node_idx in children:
            attach_parent = idx
            break
    if attach_parent is None:
        scene_index = int(gltf.get("scene", 0))
        scene_nodes = gltf["scenes"][scene_index].setdefault("nodes", [])
        attach_parent = scene_nodes[0] if scene_nodes else mesh_node_idx

    joint_node_indices = add_joint_nodes(gltf, joints, attach_parent)

    skins = gltf.setdefault("skins", [])
    skin = {
        "name": "AutoRigSkin",
        "joints": joint_node_indices,
        "inverseBindMatrices": ibm_accessor,
        "skeleton": joint_node_indices[0],
    }
    skins.append(skin)
    skin_idx = len(skins) - 1
    gltf["nodes"][mesh_node_idx]["skin"] = skin_idx

    write_glb(out_path, gltf, bin_blob)
    print(f"Autorig complete: {out_path}")
    print(f"Joints: {len(joint_node_indices)}, vertices weighted: {joint_ids.shape[0]}")


if __name__ == "__main__":
    main()

